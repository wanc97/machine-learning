# 深度学习基础知识

## 基础

**人工智能发展**

* 人工智能：泛指与智能有关的技术
* 规则学习：硬编码知识。计算机不需要从数据中学习知识。
* 机器学习：不用显式编程的学习
  * 经典的机器学习：人工设计特征。计算机从数据中学习到了“特征-->label”之间的映射。
* 特征学习：机器从数据中自动学习到特征，然后学习到了“特征 --> label”之间的映射。
* 深度学习：机器从数据中自动学习到了多层特征（深层特征由浅层特征来表达），然后学习到了“特征 --> label”之间的映射。
  * 深度学习是端到端学习，自动学习特征和映射，不用像以前一样进行手工的特征设计。但目前的发展程度还很低，人工设计网络结构一定程度上还是属于特征设计，还是需要通过先验信息来限定特征结构、模型容量。
  * 通过组合简单的概念（concept）来构建复杂的概念。如：在图片识别任务中，通过比较相邻像素的亮度，则容易地识别边缘；通过识别边的集合，则容易识别角和轮廓；通过识别轮廓和角的特点集合，则容易识别物体整体。

**数据集&参数量**

* 在大的数据集（数据中信息多）上，训练更大的网络更有可能得到更好的效果，而在小数据集上，模型的发挥空间小，效果更多取决于特征工程（人工先验知识）的能力。
* 数据量有限的情况下，模型容量并非越大越好；因为数据可以看成是用冲激函数在原分布上采样，随着模型容量的增大，一定会学到更多的噪声。
* 随着数据集的增大，一方面是数据提供的信息更多了，有利于提升泛化性能；另一方面是使得优化曲面更加平滑，降低优化难度。直观理解一个样本对于优化平面带来的是一个冲击，样本多了之后可以相互平滑，从数学上看，代价函数是多个样本的loss的平均，样本数越多时代价函数曲面更平滑，更有利于优化，但一个batch的样本数适当少一点可以加快收敛速度和提供随机性。
* 现在深度模型的参数量越来越大，但并不是参数越多越好，应该和数据量、场景相匹配，尽量用更少的参数达到相同的效果以增加模型的泛化能力。

**可辨识性**

* 如果一个训练集可以唯一确定一组模型参数，则该模型称作可辨认的。
* 带有隐变量的模型通常是不可辨认的。因为可以批量交换隐变量，从而得到等价的模型。如：交换隐单元和的权重向量。也可以放大权重和偏置倍，然后缩小输出倍，从而保持模型等价。
* 模型可辨认性问题意味着： 神经网络的代价函数具有非常多、甚至是无限多的局部极小解。 由可辨认性问题产生的局部极小解都具有相同的代价函数值，它并不是代价函数非凸性带来的问题。 假如存在一组参数使得模型、数据达到全局最小，可以通过一些变换得到无数组全局最优参数。

## 神经网络

**激活函数**

* 对输入做非线性变换，如果神经网络中没有非线性变换，则最终退化为简单线性变换（矩阵运算）；
* 一般解释sigmoid函数在深度学习上的劣势通常是从梯度消失的角度来解释，也可以尝试另一个角度：sigmoid函数就不是为深度学习而生的，它原本就是在浅层模型（逻辑斯蒂回归）中将数值从无穷区间映射到0-1之间的工具。现在这一工作基本由softmax替代，所以替代sigmoid函数的不是relu，而是softmax，另外softmax本身就是对sigmoid的扩展。Tanh几乎在所有场合都优于sigmoid函数，除非要求输出0-1，否则需要考虑sigmoid的时候完全可以用tanh取代。
* Relu函数虽然有一半的输出处于0导数，但却并不是说会有一半的神经元处于未激活的状态，因为对于不同的样本可能函数输入会很不相同，有的样本输入大于0，有的样本输入小于0，这样就不能算未激活。
* 每一个神经元都是一个特征学习器，因此怎样的激活函数更好也可以从这个角度展开分析。所以前馈网络的隐单元更适合relu，而tanh（sigmoid）用在需要限制输出的地方，如循环神经网络中防止梯度爆炸。

**先验**

* MLP就可以表达各种特征，之所以需要设计、发展各种结构，一方面是直接用MLP会参数爆炸；另一方面是在平衡困难：直接用MLP的话是把困难都推给了优化过程，因为对模型不加限制会使得很难训练到想要的参数，而设计各种结构是通过加入先验信息来限制模型能力，使得模型往想要的方向前进。
  * MLP之于神经网络，就如同高斯分布之于分布，是在没有额外先验知识时的选择，当有额外先验知识时，就应该加入先验知识，以使得模型更加符合实际。
* 学到的参数就是在表征一些模式，神经网络不同的层结构（全连接、卷积…）是不同的特征学习器，它们有着不同的先验假设，适合学习不同的特征。
* 从计算图上看，权重衰减的正则化也是一种直连，但是并没有通过权重衰减建立与标签信息的联系，只是建立与一些先验信息的联系。

## 优化

**反向传播**

* 反向传播是对多元微分的一种实现。利用动态规划的思想，用空间换时间，存储中间结果，避免链式法则中的大量重复计算。

**局部极值**

* 在极高维的时候，鞍点出现的概率要远远大于极小点，所以神经网络优化不下去很可能不是因为极小值点。
* 可以绘制梯度范数随着时间的变化：
  * 如果梯度范数没有缩小到一个很小的值，则问题的原因既不是局部极小值引起的，也不是其他形式的临界点（比如鞍点）引起的。
  * 如果梯度范数缩小到一个很小的值，则问题的原因可能是局部极小值引起的，也可能是其他原因引起的。
* 当位于函数值较低的区间时，黑塞矩阵的特征值为正的可能性更大。这意味着：
  * 具有较大函数值的临界点更可能是鞍点，因为此时黑塞矩阵的特征值可能既存在正值、也存在负值。
  * 具有较小函数值的临界点更可能是局部极小值点，因为此时黑塞矩阵的特征值更可能全部为正值。
  * 具有极高函数值的临界点更可能是局部极大值点，因为此时黑塞矩阵的特征值更可能全部为负值。
* 使用海森矩阵的优化算法需要更大的batch-size，因为海森矩阵的条件数过高，对于偏差的容忍度差。

**牛顿法**

* 梯度下降法不能保证代价函数一定下降，牛顿法更不能。
  * 梯度下降法利用一阶泰勒展开，下降要求在小领域内；
  * 牛顿法利用二阶泰勒展开，试图直接跳到极值点，在凸二次函数下可以一步达到最优，在其它凸优化情况下也可以较快达到最优，但是在非凸问题时没有保证，它的目标是尽快把每个参数都送到极值点，因此在多参数的时候大概率会跳到鞍点处，因为鞍点出现的概率要远远大于极大值点和极小值点。
  * 所以牛顿法不适合非凸优化。
* 牛顿法要应用在非凸优化时需要正则化使得海森矩阵正定，这只适合负特征值绝对值较小时，另外海森矩阵的边长等于参数量，所以在参数量极大的深度学习应用中，牛顿法的巨大计算和存储代价也是致命缺点。

**超参数调优**

* 网格搜索；随即搜索：重要的超参数用网格搜索，不重要的超参数用随机选择，以降低复杂度；
* 动态资源分配：类似于多臂老虎机问题，选择出最优的臂；目的是使得效果不好的组合可以快速被淘汰掉，随着优化的进行，不确定性减小，就可以淘汰掉更多的臂；
  * 对n组超参数组合，进行一定程度优化；
  * 保留效果最好的前一半的组合，淘汰其它；依次循环得到最优的参数组合；
* 贝叶斯优化：认为超参数存在某种分布，可以利用已经尝试的超参数对分布进行估计，进而预测收益最大的组合，进而减少实验次数；

**FTRL**

* 出发点是在线学习与模型稀疏性；在线学习应对工程场景下大模型、大数据量更新代价大的问题；稀疏性应对特征量极大的场合（推荐系统），减少特征使用量；
* 直接的解决方案是SGD+L1正则化，但是SGD的随机性无法保证在全局应该稀疏的特征在每次更新的时候都稀疏；
* FTRL与SGD+L1正则化几乎等价，可以看成是一种优化版本；
* $$w_{t+1}=argmin_w(\sum^t_{s=1}g_sw+\frac{1}{2}\sum^t_{s=1}\delta_s||w-w_s||^2_2+\lambda||w||_1)$$

#### 训练技巧

* 模型是非线性的，也无法直接求解出最优解，所以大的方向是通过梯度下降，贪心地逼近更好的解；各种策略都是服务于这个贪心的过程。
* 深度学习的方法都显得非常技巧，一个原因是因为深度学习研究的是高度非线性的系统，所以往往无法通过个别高度概括性的工具应对各种问题。所以研究深度学习需要理论结合实际，不能纸上谈兵。 **学习率**
* 基于梯度的优化算法中，每个参数都有一个最佳的学习率范围，但实际中不可能给每个参数都设置一个学习率超参数，而一个学习率又无法满足所有的参数。这个时候就有了很多的优化角度：
  * 对数据做归一化缩放操作、加批次归一化层以改善参数的一致性；
  * 自适应调整学习率；
  * 动量法自适应调整梯度（动量）等。
* 第k步的学习率记做$$ϵ_k$$：
  * $$\sum_k ϵ_k=\infty$$：保证无论多远，梯度都可以更新到；
  * $$\sum_k ϵ_k^2<\infty$$：保证更新过程稳定收敛；
* 在随机梯度下降中，学习率固定，因此随着梯度大小变化，参数更新量可能为任意数。而RMSProp动量法通过自适应的方式将参数更新量从无穷区间映射到了大约$\[-ϵ,ϵ]$（定性得到的大约区间，不一定）。
* batch size大时，优化曲面更加稳定，可以设置更大的学习率；反之batch size小时，学习率要设置的小一些；
* 周期性得让学习率随轮次减小以适应新的优化情况：
  * 指数衰减：$$\mu=\mu_0e^{-k}$$
  * 反比衰减：$$\mu=\frac{\mu_0}{1+k}$$
* 大的梯度容易跳出局部极值，但是只能学到简单的pattern，小的梯度不容易跳出局部极值，但是有利于学到复杂的pattern。在训练过程中，学习率一般是要逐步减小的，但是在陷入局部极值的时候增大学习率或许可以帮助跳出。
* 判断学习率大小，随着轮次，损失函数：
  * 下降缓慢：学习率太小，梯度下降慢；
  * 下降适中、平稳：学习率合适；
  * 下降快但很快停止下降：学习率偏大，梯度更新震荡，无法进一步下降；
  * 上升：学习率过大，梯度更新发散；
* AdaGrad：将参数历次梯度的斜边的倒数用来自适应调整学习率；梯度大的参数学习率小；很容易还没优化好就停止了；
* RMSProp：在AdaGrad的基础上，将累计求和的部分改成了指数滑动，避免自适应学习率快速衰减的问题，自适应学习率可以根据需要变大和变小；
* AdaDelta：在RMSProp的基础上，在分子上加入参数变化量的指数滑动斜边，进一步调和，让更新大的参数的学习率可以不太小，缓解自适应学习率的波动；
* Adam：在RMSProp的基础上，加入动量法，然后对动量、斜边进行一些调和，改善早期的偏差；

**早停**

* 随着训练的进行，模型基本学习了数据中的有效信息，开始拟合数据中的噪声，此时应该及时停止训练，降低过拟合；
* 随着训练的进行，泛化/测试误差先下降，后上升，在谷底时即是早停的最好时机；

**随机/批次梯度下降**

* 样本中的信息存在冗余，大量样本都对梯度做出了非常相似的贡献，使用更多样本来估计梯度的方法的收益是低于线性的；
* batch随机梯度下降中，只要没有重复使用样本，它就是真实泛化误差梯度的无偏估计；
* 计算开销更小；
* shuffle和小批次提供一定随机性，有利于跨过局部极值
  * iteration：更新一次梯度
  * epoch：所有样本用一遍

**梯度截断**

* 对梯度按阈值截断或按模截断，以避免梯度爆炸；

**特征scaling**

* 对每一个特征，将数据减去该特征的均值并除以标准差，改善不同参数的一致性，使得超曲面更加均匀一些。

**动量Momentum**

* 使用动量更新参数而不是直接用梯度，动量与上一次的动量和现在的梯度相关，或者说现在和以前的梯度共同决定；
* 通过这种方式可以调和不同参数的更新值，方向相同的梯度会变大以加快更新速度，方向反复变化的梯度会变小以降低震荡；
* Nesterov加速梯度：先用历史动量更新参数，然后计算梯度并更新，调整先后顺序；

**正则化/权重衰减/weight decay**

* 通过加入先验知识，避免参数值处于不合理的区间：
  * L2正则化：先验假设参数取值服从均值为0的高斯分布，越大的参数被打压得越多；
  * L1正则化：先验假设参数取值服从均值为0的指数分布，大小参数一起打压，可以带来参数稀疏性；
  * 同时加入L1和L2，调和两种的效果

**Dropout**

* 做法：训练时，在Dropout层按一定概率随机将一部分神经元置0，未置0的神经元值除以保留的概率；
* 原理类似：bagging+子模型参数共享；
* 未置0神经元的处理是为了使数据无偏；

**权值初始化**

* 权重初始化应该尽量保证前向传播和反向传播过程中的值得分布不发生变化，分布中的均值大都为0，而方差应该尽量不变，否则深层网络的连乘会导致梯度消失或爆炸，以及饱和等问题。
* 初始化到0无法学到任何东西，因为导致大量参数的同质，应该采用随机初始化。
* Xavier的初始化可以保证线性全连接层传递后的方差不变，但是这样对relu这样的激活函数不好，因为relu有一半的是没有值，所以方差应该大一些；

**局部正则化/Local Response Normalize**

* $$b_i=\frac{a_i}{k+\beta\sum_ia_i}$$；
* 引入竞争，更可解释，同时稳定数据分布，类似Batch Norm等标准化层效果。

**残差连接**

* 深度网络难以训练有许多原因，除了容易梯度爆炸和梯度消失外，还包括参数的依赖，高层参数靠近输出相对容易学习但是高层参数又依赖底层参数，底层参数优化不好，高层参数也无法优化；残差连接可以降低特征学习难度；
* 利于梯度回流，缓解梯度消失；

**数据增广**

* 可以看成增加模型的等变性，也可以看成抑制数据中的噪声；
* 图像的数据增广方式包括基本的旋转平移等外，还可以通过风格增广；

**门结构**

* 有助于底层信息直通

**其它**

* 各种类型的网络层与层之间的参数不应该缩减太快，因为后面的后面神经元学习的特征很难有效利用前面的众多特征。
* 参数共享也是促使模型学习主要模式，同时也可以起到抑制过拟合的作用，因为有限的参数要尽量满足主要特征，就自然忽略了噪声。

#### 标准化层

* 一说可以防止数据分布的变化（内部协变量偏移）带来性能下降；另一说模型的优化曲面发生变化，变得更加平滑，非凸性减弱，使得训练变得容易，即利普希茨系数严格降低；

**Batch Norm**

* Batch Norm：在对一个通道内，对一批次数据的每个特征，减均值除标准差，同时再加入一对可学习的均值方差参数。
  * 如果不是卷积网络，Batch Norm也可以是对于单个神经元的;
  * 训练过程使用的减均值除方差是通过一批的训练数据得到，测试的是通过训练过程的指数滑动平均得到;
  * 通过Batch Norm，深度模型更好地收敛训练，即使对于比较大的学习率也有较好的效果;
  * 在batch size较大的时候，Batch分布与整体分布类似，Batch Norm的效果往往好于Other Norm；同时Batch Norm高度依赖于mini batch 的大小。它要求每个mini-batch 都比较大，因此不适合batch size 较小的场景，如：在线学习（batch size=1 ）；
  * 不适合RNN网络，因为不同样本的sequence的长度不同，因此RNN的深度是不固定的。同一个batch中的多个样本会产生不同深度的RNN，因此很难对同一层的样本进行归一化；
* 理解Batch Norm层的一个角度：
  * 原来的网络就像一大段代码叠在一起，可读性差（层与层之间相互依赖，难以训练）；
  * 加上Batch Norm层之后就像把以前的代码分割成了一个个子程序，依此调用，程序之间有清晰的接口，因此阅读和调试都更加容易；
  * 这就像是一种解耦操作，让各个层专注自己的任务（特征学习）。
  * 同时也是在改善参数的一致性。
* 在激活函数之前、还是之后进行Batch Norm都可，在激活函数之前进行的更常见；
* Batch Norm层前的全连接层一般不需要带偏差项，因为偏差项会被吸走，卷积层可以带。

**Layer Norm**

* 因为BN是在批次维度进行，所以叫批次标准化，还可以在其他维度进行，得到不同种类的标准化。
* Layer Norm不依赖于batch size，适合在线学习，也适合于RNN网络。

**Instance Norm**

* 对于GAN、风格迁移这类任务上，Instance Norm效果要优于BN，因为每张图片自己的风格比较独立，不应该与batch中其它图片产生太大联系。
* Instance Norm也不依赖于batch size，适合在线学习，也适合于RNN网络。

#### 卷积神经网络

* 在MLP的基础上增加了两个先验：局部性连接 & 参数共享。
* 这两个先验假设使得卷积网络相对于MLP的参数量大减，泛化能力更加强，可以更好地学习到图像数据中的重要模式（主要指像素之间的变化关系、组合关系，所以非常适合计算视觉领域，当然也可以用在同样具有类似重要特征组合方式的领域）。
* 卷积和互相关都是由函数到函数的算子，其中卷积包括反转和求和两个过程，互相关只有求和；所以卷积神经网络是披着卷积外衣的互相关网络。
* 卷积核的大小也会影响卷积核的表达能力，参数量小的卷积核所能产生的可能性也少。

> * 卷积层：提取数据组合特征；
> * 池化层：降低数据量；
> * 1\*1卷积：增加非线性，降低参数量；
> * 1\*n卷积：降低参数量；
> * Dropout：类似bagging，集成学习的效果，降低方差；
> * 数据增广：变相增加数据量，抵消不重要信息，加强核心成分，避免模型学偏；
> * ReLU：缓解梯度消失；
> * 多尺度卷积核：学习不同尺度特征；
> * 空洞卷积：多尺度+减少参数量；
> * BN：应对数据分布漂移，避免数据整体进入饱和区，进而缓解梯度消失和梯度爆炸的问题；
> * 不变性：$$f(T(x))=f(x)$$，池化层带来一定不变性；
> * 等变性：$$f(T(x))=T(f(x))$$，卷积层参数共享带来一定的平移等变性，等变性可以看成是对数据噪声的一种抑制，如图片分类中无论物体在图片的中央还是边缘，都应该得到一样的结果，这种位置信息就是和分类无关的噪声；
> * 全局平均池化：提供平移不变性，丢失很多细节
> * 分支训练：网络分叉，如GoogLeNet，可以将梯度信息更好的传递到底层，防止梯度消失。

* 卷积核参数均值影响特征图亮度，值相加和为1时处理之后的图像与原始图像的亮度相比几乎一致，小于1时减小，大于1时增大；

#### 循环神经网络

* RNN在MLP的基础上加上（不同时刻/序列点）参数共享的先验。
* RNN在前向传播和反向传播的时候都有矩阵乘幂，序列略长就必然崩溃，实用性非常低，主要是思想重要。
* 双向循环神经网络假设上下文都会对当下有影响，因此从两个方向传递信息；时序数据中，后面时刻的数据对前面时刻的数据没有因果关系，但是可以有相关关系；
* LSTM在RNN上加入门控逻辑（遗忘、输入、输出），选择性遗忘和更新信息，可以应用在序列不太长的情况；
* GRU整合门数量（更新、复位）并进一步减少计算量（不区分cell和h），优化结构。
* attention机制可以解决长时间依赖的问题：不再去记录中间信息，而是学习捕获数据间的联系，因此不受长序列信息丢失的影响；
* 循环神经网络中sigmoid的0～1区间用于模拟门的开闭；tanh用于信息激活函数，线性区间大于sigmoid，同时防止relu可能带来的信息爆炸；
* 编码-解码架构可以应对seq2seq问题中输入输出序列长度不一致的情况；

#### 图神经网络

**图**

* 图的组成：顶点，边
* 有向图：边有方向。
* 无向图：边没有方向，可以看成有向图的边成对出现。
* 带权图：边有权重。

**度**

* 有向图度：
  * 入度：指向自己的边数
  * 出度：自己发出的边数
* 无向图度：连接边的数量

**邻接矩阵&临接表**

* 邻接矩阵是方阵，宽度等于节点数，元素为1代表节点之间有边连接，邻接矩阵浪费空间；每一行相当于一个节点的onehot编码
* 无向图的邻接矩阵是对称阵，有向图不一定。
* 邻接表节省空间但效率低，cache不友好。

**子图** 节点和边都是一个大图的子集。

**连通性**

* 连通图：无向图中所有节点都可以连接在一起。
* 连通分量：连通子图数称，连通图的连通分量为1。
* 强连通图：有向图任意节点可以相互到达
* 弱连通图：有向图不是强连通图，但是作为无向图时是连通图。

**最短路径**

* 两个节点之间的最短距离，可能是边数，如果边权重不一样就是最短的边的和。
* 图直径：图所有节点的最短路径的最大值

**节点重要性**

* 度中心性：节点度数/(n-1)；即度越高，度中心性越高。
* 特征向量中心性：对邻接矩阵求特征值、特征向量，最大特征值对应的特征向量就是各个节点的特征向量中心性。
  * 不只看边数，还可以反映连接节点的重要性。
* 中介中心性：经过自己的最短路径数/总的最短路径数
  * 如果有的节点对最短路径可以有多种走法，则分数平均。
* 连接中心性：(n-1)/该节点到其它节点的最短路径之和。
  * 即节点越靠中心，分母越小，值越大。

**PageRank**

* 边的PageRank值为源节点的PageRank除以出度，节点的PageRank值等于入边的PageRank值之和；交替计算直至稳定。

```python
import networkx as nx

# 可以从其它的数据结构中导入，如pandas邻接表
G = nx.from_..()  
nx.degree(G)
nx.connected_components(G)
......
nx.pagerank(G)
......
```

**拉普拉斯矩阵**

* $$L=D-A$$，其中$$A$$为图的邻接矩阵，$$D$$为图的度矩阵（对角）；对拉普拉斯矩阵进行特征值分解可以得到一系列特征值和对应的特征向量。

**图卷积**

* 卷积是将数据从一个域转换到另一个域中，如信号处理领域中从时域到频域，图像处理中从图片到特征图，图领域中从图到对应特征向量；图像中的每一个特征图是对图像的一种分解，图中的特征向量也是对图连接关系的一种分解。
  * DEFFERRARD M, BRESSON X, VANDERGHEYNST P. Convolutional neural networks on graphs with fast localized spectral filtering\[C]//Advances in neural information processing systems. 2016: 3844–3852.
* 在图结构的数据上进行特征提取，数据挖掘；GCN是CNN在图数据上的扩展，将卷积特征提取方式从欧几里得空间迁移到非欧几里得空间。
  * BATTAGLIA P W, HAMRICK J B, BAPST V, 等. Relational inductive biases, deep learning, and graph networks\[J]. arXiv preprint arXiv:1806.01261, 2018.
  * BRONSTEIN M M, BRUNA J, LECUN Y, 等. Geometric deep learning: going beyond euclidean data\[J]. IEEE Signal Processing Magazine, IEEE, 2017, 34(4): 18–42.
