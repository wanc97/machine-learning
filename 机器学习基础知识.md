<!-- TOC -->

- [机器学习中不确定性有三个来源:](#机器学习中不确定性有三个来源)
- [没有免费的午餐定理(No Free Lunch Theorem:NFL)：](#没有免费的午餐定理no-free-lunch-theoremnfl)
- [特征空间](#特征空间)
    - [输入空间 ：](#输入空间-)
    - [输出空间 ：](#输出空间-)
    - [特征空间](#特征空间-1)
- [机器学习的对象是：](#机器学习的对象是)
- [机器学习任务类型：](#机器学习任务类型)
    - [监督学习任务：](#监督学习任务)
    - [无监督学习任务：](#无监督学习任务)
    - [半监督学习任务：](#半监督学习任务)
    - [强化学习任务：](#强化学习任务)
- [机器学习算法类型：](#机器学习算法类型)
    - [传统统计学习：](#传统统计学习)
    - [深度学习：](#深度学习)
- [独立同分布](#独立同分布)
- [概率模型或者非概率模型：](#概率模型或者非概率模型)
- [机器学习三要素：](#机器学习三要素)
- [经验风险最小化&结构风险最小化&奥卡姆剃刀原理](#经验风险最小化结构风险最小化奥卡姆剃刀原理)
- [概率&似然](#概率似然)
- [P-value](#p-value)
- [偏差&方差](#偏差方差)
    - [误差可以分解为偏差、方差和噪声之和：](#误差可以分解为偏差方差和噪声之和)
- [过拟合](#过拟合)
    - [缓解过拟合](#缓解过拟合)
- [假设空间](#假设空间)
- [VC维理论](#vc维理论)
- [泛化能力评估](#泛化能力评估)
- [性能度量](#性能度量)
    - [混淆矩阵](#混淆矩阵)
    - [P-R曲线](#p-r曲线)
    - [ROC曲线](#roc曲线)

<!-- /TOC -->

### 机器学习中不确定性有三个来源:
- 模型本身固有的随机性。如：量子力学中的粒子动力学方程。

- 不完全的观测。即使是确定性系统，当无法观测所有驱动变量时，结果也是随机的。

- 不完全建模。有时必须放弃一些观测信息。如机器人建模中：虽然可以精确观察机器人周围每个对象的位置，但在预测这些对象将来的位置时，对空间进行了离散化。则位置预测将带有不确定性。

### 没有免费的午餐定理(No Free Lunch Theorem:NFL)：
- 对于一个学习算法A，如果在某些问题上它比算法B好，那么必然存在另一些问题，在那些问题中B比A更好。

- 因此不存在这样的算法：它在所有的问题上都取得最佳的性能。因此要谈论算法的优劣必须基于具体的学习问题。

### 特征空间
#### 输入空间 ：
- 所有输入的可能取值；

#### 输出空间 ：
- 所有输出的可能取值。

- 特征向量表示每个具体的输入， 所有特征向量构成特征空间。

#### 特征空间
- 特征空间是指经过特征工程处理过的输入空间

- 特征空间的每一个维度对应一种特征。

- 可以将输入空间等同于特征空间，但是也可以不同。绝大多数情况下，输入空间等于特征空间。

- 模型是定义在特征空间上的。

### 机器学习的对象是：
- 具有一定的统计规律的数据。

### 机器学习任务类型：
#### 监督学习任务：
- 从已标记的训练数据来训练模型。 主要分为：分类任务、回归任务、序列标注任务。

- 分类和回归的区别主要在于标签是连续值还是离散值

#### 无监督学习任务：
- 从未标记的训练数据来训练模型。主要分为：聚类任务、降维任务。

#### 半监督学习任务：
- 用大量的未标记训练数据和少量的已标记数据来训练模型。

#### 强化学习任务：
从系统与环境的大量交互知识中训练模型。

### 机器学习算法类型：
#### 传统统计学习：
- 基于数学模型的机器学习方法。包括SVM、逻辑回归、决策树等。

- 这一类算法基于严格的数学推理，具有可解释性强、运行速度快、可应用于小规模数据集的特点。

#### 深度学习：
- 基于神经网络的机器学习方法。包括前馈神经网络、卷积神经网络、递归神经网络等。

- 这一类算法基于神经网络，可解释性较差，强烈依赖于数据集规模。这类算法在语音、视觉、自然语言等领域非常成功。

### 独立同分布
- 监督学习假设输入与标记遵循一个联合概率分布，训练数据和测试数据依联合概率分布进行独立同分布产生。

- 学习过程中，假定这个联合概率分布存在，但是具体定义未知。

### 概率模型或者非概率模型：
- 监督学习的模型可以为概率模型或者非概率模型：
- 概率模型由条件概率分布表示。
- 非概率模型由决策（判别）函数表示

### 机器学习三要素：
- 模型、策略、算法

### 经验风险最小化&结构风险最小化&奥卡姆剃刀原理
- **经验风险**最小化的目的是使模型更好地拟合数据样本；

- **结构风险**是在经验风险最小的基础上加入先验知识，使得模型往更加合理的方向优化；

- 结构风险最小化策略符合**奥卡姆剃刀原理**：能够很好地解释已知数据，且十分简单才是最好的模型。

- **极大似然估计**就是经验风险最小化的例子

- **最大后验估计**就是结构风险最小化的例子

### 概率&似然
- 概率是描述在确定分布下某个事件发生可能性的大小；

- 似然是描述在发生某些事件的情况下为某个分布的概率。

### P-value
- 一个事件的P-value不是这个事件的概率值，而是所有概率小于等于该事件概率值得概率和（积分），即反应该事件发生的稀有程度。

### 偏差&方差
- 偏差描述模型对于特定数据（训练集）的拟合效果；
- 方差描述模型在不同数据（训练集与测试集）上拟合效果的差别；
- 欠拟合：高偏差，低方差；
- 过拟合：低偏差，高方差。

#### 误差可以分解为偏差、方差和噪声之和：
- 偏差：度量了学习算法的期望预测与真实结果之间的偏离程度，刻画了学习算法本身的拟合能力。
- 方差：度量了训练集的变动所导致的学习性能的变化，刻画了数据扰动造成的影响。
- 噪声：度量了在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度。
- 偏差-方差分解表明：泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。
- 偏差-方差分解中，噪声也可以称作最优误差或者贝叶斯误差。如：在图像识别的问题中，人眼识别的错误率可以视作最优误差。


### 过拟合
- 过拟合的原因是：将训练样本本身的一些特点当作了所有潜在样本都具有的一般性质，这会造成泛化能力下降。
- 过拟合无法避免，只能缓解。因为机器学习的问题通常是NP难甚至更难的，而有效的学习算法必然是在多项式时间内运行完成。如果可以避免过拟合，这就意味着构造性的证明了P=NP 。

#### 缓解过拟合
- 正则化：限制不合适的假设空间；在不同的问题中，正则化项可以有不同的形式：
> - 回归问题中，损失函数是平方损失，正则化项是参数向量的P范数。
> - 贝叶斯估计中，正则化项对应于模型的先验概率。

- 数据集增广：通过人工规则产生虚假数据来创造更多的训练数据。
- 噪声注入：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数中。
- 早停：当验证集上的误差没有进一步改善时，算法提前终止。

### 假设空间
- 代表模型的函数集合。这也称作模型的表示容量representational capacity。
- 由于额外的限制因素（比如优化算法的不完善），模型的有效容量effective capacity一般会小于模型的表示容量
- 通常在模型的假设空间中出最佳的函数是非常困难的优化问题，实际应用中只是挑选一个使得训练误差足够低的函数即可。

### VC维理论
- 训练误差与泛化误差之间差异的上界随着模型容量增长而增长，随着训练样本增多而下降 。
- VC 维理论对于机器学习算法有很好的指导作用，但是它在深度学习很难应用。因为边界太宽泛，且难以确定深度学习的容量。由于深度学习模型的有效容量受限于优化算法，因此确定深度学习模型的容量特别困难。

### 泛化能力评估
- 留出法：直接将数据切分为三个互斥的部分（也可以切分成两部分，此时训练集也是验证集），然后在训练集上训练模型，在验证集上选择模型，最后用测试集上的误差作为泛化误差的估计。
- K折交叉验证法：数据随机划分为K个互不相交且大小相同的子集，利用K-1个子集数据训练模型，利用余下的一个子集测试模型。
- 留一法：假设数据集中存在N个样本，令K=N则得到了K折交叉验证的一个特例。
- 自助采样法：放回采样，随机森林选用此策略，会改变数据分布。

### 性能度量
#### 混淆矩阵
| 真实/预测 | 正类 | 反类 |
| --- | --- | --- |
| 正类 | TP | FN |
| 反类 | FP | TN |

- 查准率(precision)：$P=\frac{TP}{TP+FP}$

- 查全率(recall):$P=\frac{TP}{TP+FN}$

- 查准率和查全率都是越大越好；但是对于已有模型，这两个值是一对负相关的；他们的大小由区分正反类的阈值确定，阈值设得越高时模型越倾向于预测为反类，此时recall下降，precision上升，反之亦然。

#### P-R曲线
调整阈值可以得到不同的precision-recall对，进而得到P-R曲线。

- P-R曲线从左上角(0,1) 到右下角(1,0) 。

- 开始时第一个样本（最可能为正例的）预测为正例，其它样本都预测为负类。此时：查准率很高，几乎为1；查全率很低，几乎为0，大量的正例没有找到；

- 结束时所有的样本都预测为正类。此时：查全率很高，正例全部找到了，查全率为1；查准率很低，大量的负类被预测为正类。

#### ROC曲线
- 正确报警率（真正例率）：$TPR=\frac{TP}{TP+FN}$
- 误警率（假正例率）：$FPR=\frac{FP}{FP+TN}$ 

- ROC曲线从左下角(0,0) 到右上角(1,1) 。

- 开始时第一个样本（最可能为正例的）预测为正例，其它样本都预测为负类。此时：真正例率很低，几乎为0，因为大量的正例未预测到；假正例率很低，几乎为0，因为此时预测为正类的样本很少，所以几乎没有错认的正例；

- 结束时所有的样本都预测为正类。此时：真正例率很高，几乎为1，因为所有样本都预测为正类；假正例率很高，几乎为1，因为所有的负样本都被错认为正类。

- 对角线对应于随机猜想模型。点(0,1)对应于理想模型；通常ROC曲线越靠近点(0,1)越好。

P-R曲线和ROC曲线上的每一个点都对应了一个阈值的选择，该点就是在该阈值下的(查准率，查全率) /(真正例率，假正例率) 。

