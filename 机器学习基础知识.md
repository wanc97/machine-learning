### 机器学习中不确定性有三个来源:
模型本身固有的随机性。如：量子力学中的粒子动力学方程。

不完全的观测。即使是确定性系统，当无法观测所有驱动变量时，结果也是随机的。

不完全建模。有时必须放弃一些观测信息。

如机器人建模中：虽然可以精确观察机器人周围每个对象的位置，但在预测这些对象将来的位置时，对空间进行了离散化。则位置预测将带有不确定性。

### 没有免费的午餐定理(No Free Lunch Theorem:NFL)：
对于一个学习算法A，如果在某些问题上它比算法B好，那么必然存在另一些问题，在那些问题中B比A更好。

因此不存在这样的算法：它在所有的问题上都取得最佳的性能。因此要谈论算法的优劣必须基于具体的学习问题。

### 特征空间
#### 输入空间 ：
所有输入的可能取值；
#### 输出空间 ：
所有输出的可能取值。

特征向量表示每个具体的输入， 所有特征向量构成特征空间。

#### 特征空间
特征空间是指经过特征工程处理过的输入空间

特征空间的每一个维度对应一种特征。

可以将输入空间等同于特征空间，但是也可以不同。绝大多数情况下，输入空间等于特征空间。

模型是定义在特征空间上的。

### 机器学习的对象是：
具有一定的统计规律的数据。

### 机器学习任务类型：
#### 监督学习任务：
从已标记的训练数据来训练模型。 主要分为：分类任务、回归任务、序列标注任务。

分类和回归的区别主要在于标签是连续值还是离散值
#### 无监督学习任务：
从未标记的训练数据来训练模型。主要分为：聚类任务、降维任务。
#### 半监督学习任务：
用大量的未标记训练数据和少量的已标记数据来训练模型。
#### 强化学习任务：
从系统与环境的大量交互知识中训练模型。

### 机器学习算法类型：
#### 传统统计学习：
基于数学模型的机器学习方法。包括SVM、逻辑回归、决策树等。

这一类算法基于严格的数学推理，具有可解释性强、运行速度快、可应用于小规模数据集的特点。

#### 深度学习：
基于神经网络的机器学习方法。包括前馈神经网络、卷积神经网络、递归神经网络等。

这一类算法基于神经网络，可解释性较差，强烈依赖于数据集规模。但是这类算法在语音、视觉、自然语言等领域非常成功。

### 独立同分布
监督学习假设输入与标记遵循一个联合概率分布，训练数据和测试数据依联合概率分布进行独立同分布产生。

学习过程中，假定这个联合概率分布存在，但是具体定义未知。

### 概率模型或者非概率模型：
监督学习的模型可以为概率模型或者非概率模型：
概率模型由条件概率分布表示。
非概率模型由决策（判别）函数表示

### 机器学习三要素：
模型、策略、算法

### 经验风险最小化&结构风险最小化&奥卡姆剃刀原理
**经验风险**最小化的目的是使模型更好地拟合数据样本；

**结构风险**是在经验风险最小的基础上加入先验知识，使得模型往更加合理的方向优化；

结构风险最小化策略符合**奥卡姆剃刀原理**：

能够很好的解释已知数据，且十分简单才是最好的模型。

**极大似然估计**就是经验风险最小化的例子

**最大后验估计**就是结构风险最小化的例子







