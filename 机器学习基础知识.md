### 机器学习中不确定性有三个来源:
- 模型本身固有的随机性。如：量子力学中的粒子动力学方程。

- 不完全的观测。即使是确定性系统，当无法观测所有驱动变量时，结果也是随机的。

- 不完全建模。有时必须放弃一些观测信息。如机器人建模中：虽然可以精确观察机器人周围每个对象的位置，但在预测这些对象将来的位置时，对空间进行了离散化。则位置预测将带有不确定性。

### 没有免费的午餐定理(No Free Lunch Theorem:NFL)：
- 对于一个学习算法A，如果在某些问题上它比算法B好，那么必然存在另一些问题，在那些问题中B比A更好。

- 因此不存在这样的算法：它在所有的问题上都取得最佳的性能。因此要谈论算法的优劣必须基于具体的学习问题。

### 特征空间
#### 输入空间 ：
- 所有输入的可能取值；

#### 输出空间 ：
- 所有输出的可能取值。

- 特征向量表示每个具体的输入， 所有特征向量构成特征空间。

#### 特征空间
- 特征空间是指经过特征工程处理过的输入空间

- 特征空间的每一个维度对应一种特征。

- 可以将输入空间等同于特征空间，但是也可以不同。绝大多数情况下，输入空间等于特征空间。

- 模型是定义在特征空间上的。

### 机器学习的对象是：
- 具有一定的统计规律的数据。

### 机器学习任务类型：
#### 监督学习任务：
- 从已标记的训练数据来训练模型。 主要分为：分类任务、回归任务、序列标注任务。

- 分类和回归的区别主要在于标签是连续值还是离散值

#### 无监督学习任务：
- 从未标记的训练数据来训练模型。主要分为：聚类任务、降维任务。

#### 半监督学习任务：
- 用大量的未标记训练数据和少量的已标记数据来训练模型。

#### 强化学习任务：
从系统与环境的大量交互知识中训练模型。

### 机器学习算法类型：
#### 传统统计学习：
- 基于数学模型的机器学习方法。包括SVM、逻辑回归、决策树等。

- 这一类算法基于严格的数学推理，具有可解释性强、运行速度快、可应用于小规模数据集的特点。

#### 深度学习：
- 基于神经网络的机器学习方法。包括前馈神经网络、卷积神经网络、递归神经网络等。

- 这一类算法基于神经网络，可解释性较差，强烈依赖于数据集规模。但是这类算法在语音、视觉、自然语言等领域非常成功。

### 独立同分布
- 监督学习假设输入与标记遵循一个联合概率分布，训练数据和测试数据依联合概率分布进行独立同分布产生。

- 学习过程中，假定这个联合概率分布存在，但是具体定义未知。

### 概率模型或者非概率模型：
- 监督学习的模型可以为概率模型或者非概率模型：
- 概率模型由条件概率分布表示。
- 非概率模型由决策（判别）函数表示

### 机器学习三要素：
- 模型、策略、算法

### 经验风险最小化&结构风险最小化&奥卡姆剃刀原理
- **经验风险**最小化的目的是使模型更好地拟合数据样本；

- **结构风险**是在经验风险最小的基础上加入先验知识，使得模型往更加合理的方向优化；

- 结构风险最小化策略符合**奥卡姆剃刀原理**：能够很好的解释已知数据，且十分简单才是最好的模型。

- **极大似然估计**就是经验风险最小化的例子

- **最大后验估计**就是结构风险最小化的例子

### 概率&似然
- 概率是描述在确定分布下某个事件发生可能性的大小；

- 似然是描述在发生某些事件的情况下为某个分布的概率。

### P-value
- 一个事件的P-value不是这个事件的概率值，而是所有概率小于等于该事件概率值得概率和（积分），即反应该事件发生的稀有程度。

### 偏差&方差
- 偏差描述模型对于特定数据（训练集）的拟合效果；

- 方差描述模型在不同数据（训练集与测试集）上拟合效果的差别。

- 欠拟合：高偏差，低方差；

- 过拟合：低偏差，高方差。




