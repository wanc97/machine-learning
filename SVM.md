## 特征空间维度
- 因为核技巧隐藏了映射的细节，只知道内积运算，所以特征空间是希尔伯特空间，维度是无限的。

- 但是仅从决策函数看，也可以说特征空间的维度是有限的且等于训练样本的数量，每一个训练样本对应一个维度，决策时计算出测试点在各个维度上的得分累加进行分类。

- 非线性SVM可以看成是一种特殊的神经网络，即输入层对应输入空间，隐藏层对应特征空间，判决分数对应输出层。各层神经元个数分别为输入数据维度、训练样本（或支持向量）维度、1。如果核函数选的合适就是一个广度网络。

- 另外希尔伯特空间本身就是去掉维度的概念，所以也可以说是没有维度。

## 理论上SVM的目标函数可以使用梯度下降法来训练。但存在三个问题：

- 合页损失函数部分不可导。这可以通过sub-gradient descent 来解决。
- 收敛速度非常慢。
- 无法得出支持向量和非支持向量的区别。

## 常用核函数：
- 多项式核函数；
- 高斯核函数；

## 支持向量机的优点：

- 有严格的数学理论支持，可解释性强。
- 能找出对任务至关重要的关键样本（即：支持向量）。
- 采用核技巧之后，可以处理非线性分类/回归任务。

## 支持向量机的缺点：

- 训练时间长。
- 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为平方。
- 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。
- 因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。









### 参考：
-  http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/2_svm.html

