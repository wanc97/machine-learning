{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost 过拟合问题：\n",
    "### 当AdaBoost 的基础分类器比较复杂时，AdaBoost 很容易陷入过拟合。\n",
    "\n",
    "### 但是当AdaBoost 的基础分类器比较简单时，AdaBoost 反而难以陷入过拟合。这也是为什么AdaBoost 的基础分类器经常选择使用树桩的原因。\n",
    "### 原因可能是因为简单的基础分类器只能学到简单的特征、简单的决策边界，相当于自带正则化效果，因此不容易过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从偏差-方差分解的角度来看：\n",
    "\n",
    "### Bagging主要关注降低方差，它能平滑强学习器的方差。\n",
    "\n",
    "因此它在非剪枝决策树、神经网络等容易受到样本扰动的学习器上效果更为明显。\n",
    "\n",
    "### Boosting 主要关注降低偏差，它能将一些弱学习器提升为强学习器。\n",
    "\n",
    "因此它在SVM 、knn 等不容易受到样本扰动的学习器上效果更为明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET或Extra-Trees（Extremely randomized trees，极端随机树）算法与随机森林算法十分相似，都是由许多决策树构成。极限树与随机森林的主要区别：\n",
    "\n",
    "randomForest应用的是Bagging模型，extraTree使用的所有的样本，只是特征是随机选取的，因为分裂是随机的，所以在某种程度上比随机森林得到的结果更加好\n",
    "\n",
    "随机森林是在一个随机子集内得到最佳分叉属性，而ET是完全随机的得到分叉值，从而实现对决策树进行分叉的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
