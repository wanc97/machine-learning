# 数据预处理
## 唯一属性
有一些特征如id是每个样本都不同，这样的特征对于刻画样本自身属性没有帮助，应该直接删掉。

## 缺失值
- 直接使用带缺失值的样本，如决策树类的少量算法可以直接将缺失值作为一种情况进行处理。

- 直接删除有缺失值的样本，简单，数据纯净，但是浪费了一些信息，在缺失值较多的场合不适用。

- 缺失值补全，可以进而能够尽量全面的利用信息，但是补全方式不恰当时效果适得其反。

## 缺失值补全：
- 均值插补：连续特征用均值，离散特征用众数；

- 同类均值插补：按类别进行均值插补；

- 建模预测：建立模型预测，缺点是，如果预测的特征没有关联那预测的结果就没有意义，如果有关联预测出来的信息也是冗余的，也没有意义，所以用的不多。

- 高维映射：离散特征可以用独热码表示，连续特征先离散化，好处是较为全面的保留了信息，缺点是耗费空间；

## 特征编码
### 特征二元化
将数值型特征转化成布尔型特征，通过一个阈值超参数划分特征

### 独热码
- 可以处理非数值特征；
- 降低单个特征的重要性；
- 对于有大小关系的数值变量，用独热码表示会丢失信息；

### 离散化
- 分桶

## 数据标准化、正则化
### 数据标准化
- min-max标准化
- z-score标准化

注：划分的训练集、测试集等需要使用相同的标准进行处理

### 数据正则化
对每个样本将p范数放缩到1，以便于求样本相似度等运算

标准化是对特征的操作，正则化是对样本的操作

## 特征选择
### 不进行特征选择的坏处
- 相对样本量小，维度灾难，模型泛化能力弱

- 计算量大

- 无关特征误导训练方向

## 稀疏表示和字典学习
对样本$\vec{\mathbf{x}}_{i}$通过交替寻优的方式学习字典$\mathbf{B}$和稀疏表示$\vec{\alpha}_{i}$，优化目标为：

$$
\min _{\vec{\alpha}_{i}}|| \overrightarrow{\mathbf{x}}_{i}-\mathbf{B} \vec{\alpha}_{i}||_{2}^{2}+\lambda \sum_{i=1}^{N}|| \vec{\alpha}_{i}||_{1}
$$

## 多分类问题
将二分类模型用于多分类问题：
- 一对多，为每一个类别训练一个分类器，非常容易陷入样本不平衡；
- 一对一，为每一对类别训练一个分类器，计算量太大；
- 多对多，每次都将若干个类作为正类，若干个其他类作为反类。

## 类别不平衡问题
- 对多的样本欠采样，可能丢失一些重要信息，常用方法是将反类划分成若干个集合供不同学习器使用，这样对每个学习器来看都是欠采样，但是全局来看并不会丢失重要信息。

- 对少的样本过采样，SMOTE方法：对于一个样本，从其同类近邻中随机选取一个点，在这两个点之间随机插值。

- 极不平衡时将问题看作单分类问题或异常检测。

## 参考
- http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/8_feature_selection.html
