## AdaBoost 过拟合问题：
- 当AdaBoost 的基础分类器比较复杂时，AdaBoost 很容易陷入过拟合。
- 但是当AdaBoost 的基础分类器比较简单时，AdaBoost 反而难以陷入过拟合。这也是为什么AdaBoost 的基础分类器经常选择使用树桩的原因。
- 原因可能是因为简单的基础分类器只能学到简单的特征、简单的决策边界，相当于自带正则化效果，因此不容易过拟合。

## 从偏差-方差分解的角度来看：

- Bagging主要关注降低方差，它能平滑强学习器的方差。
 - 因此它在非剪枝决策树、神经网络等容易受到样本扰动的学习器上效果更为明显。

- Boosting 主要关注降低偏差，它能将一些弱学习器提升为强学习器。
 - 因此它在SVM 、knn 等不容易受到样本扰动的学习器上效果更为明显。

## ET或Extra-Trees（Extremely randomized trees，极端随机树）算法与随机森林算法十分相似，都是由许多决策树构成。极限树与随机森林的主要区别：
- randomForest应用的是Bagging模型，extraTree使用的所有的样本，只是特征是随机选取的，因为分裂是随机的，所以在某种程度上比随机森林得到的结果更加好
- 随机森林是在一个随机子集内得到最佳分叉属性，而ET是完全随机的得到分叉值，从而实现对决策树进行分叉的。








