### 信息论背后的原理是

- 从不太可能发生的事件中能学到更多的有用信息。

即：

- 发生可能性较大的事件包含较少的信息。
- 发生可能性较小的事件包含较多的信息。
- 独立事件包含额外的信息 。

### 自信息self-information

对于事件 ，定义自信息self-information为该事件发生概率的负对数，即概率越大的事件信息越少。

自信息仅仅处理单个输出，但是如果计算自信息的期望，它就是熵。

#### 熵函数

用来对q这个分布的不确定性进行编码所需的信息量；

#### 相对熵（KL散度）

给定分布q之后，还需要多少信息来编码分布p；

- 假如两个分布一样，相对熵就是0；
- 两个分布完全不一样时相对熵为无穷大；

#### 交叉熵

度量两个分布的距离，对应最大似然估计。范围为真实分布的熵到无穷大；

ceshi