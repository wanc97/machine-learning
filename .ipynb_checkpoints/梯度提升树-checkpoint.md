# 梯度提升树
- 利用boosting的原理不断训练新的树来拟合已有模型的残差。

## 正则化
### 学习率
- 不直接使用新训练的树来更新模型，而是会加上一个较小的学习率来提高泛化误差。

### 子集采样
- 从原始数据集中不放回采样一个一个子集，引入随机性。

### 最小叶节点样本数
- 限制叶节点至少有的样本数量，低于该数量事直接停止训练。

# xgboost
相对于基本的梯度梯度提升树

- 将目标函数进行二阶泰勒展开，而非只使用一阶信息；

- 使用结构风险最小化，考虑节点数量和节点值大小；

## 分解结点
### 基本思路
循环遍历所有特征的所有分裂点，计算能够带来的增益，选择增益最大的情况对结点进行分裂，如果增益都不大于0则停止。

### 近似算法（优化）
#### 分桶：
减少分裂次数，连续特征按连续值百分位分桶，离散特征按离散值分桶；

**全局模式**：在算法开始时，对每个维度分桶一次，后续的分裂都依赖于该分桶并不再更新。

优点是：只需要计算一次，不需要重复计算。

缺点是：在经过多次分裂之后，叶结点的样本有可能在很多全局桶中是空的。

**局部模式**：除了在算法开始时进行分桶，每次拆分之后再重新分桶。

优点是：每次分桶都能保证各桶中的样本数量都是均匀的。
缺点是：计算量较大。

## 正则化
- 学习率
- 随机选取特征

## 计算速度提升
### 预排序
在程序开始的时候对数据在每个特征，按数据的大小进行排序，这样在训练的时候就不用排序，可以避免大量重复劳动，同时因为每个特征之间是独立的，因此在寻找划分点的时候就可以并行执行。

# LightGBM
## 比较
### GBT 的缺点：
- 在构建子决策树时为了获取分裂点，需要在所有特征上扫描所有的样本，从而获得最大的信息增益。

- 当样本的数量很大，或者样本的特征很多时，效率非常低。

- 同时GBT也无法使用类似mini batch方式进行训练。

### xgboost 缺点：
- 每轮迭代都需要遍历整个数据集多次。

- 如果把整个训练集装载进内存，则限制了训练数据的大小。如果不把整个训练集装载进内存，则反复读写训练数据会消耗非常大的IO 时间。

- 空间消耗大。预排序（pre-sorted）需要保存数据的feature 值，还需要保存feature 排序的结果（如排序后的索引，为了后续的快速计算分割点）。因此需要消耗训练数据两倍的内存。

- 时间消耗大。为了获取分裂点，需要在所有特征上扫描所有的样本，从而获得最大的信息增益，时间消耗大。

- 对cache 优化不友好，造成cache miss 。预排序后，feature 对于梯度的访问是一种随机访问，并且不同feature 访问的顺序不同，无法对cache 进行优化。

### LightGBM 的优点：
- 更快的训练效率；

- 低内存使用；

- 更高的准确率；

- 支持并行化学习；

- 可处理大规模数据。

## 策略：
减少训练样本的数量和减少样本的训练特征数量。

### Gradient-based One-Side Sampling(GOSS)： 
基于梯度的采样。该方法用于减少训练样本的数量。

- 传统采样方法采用随机丢弃的策略，而GOSS方法保留梯度较大的样本，随机丢弃梯度较小的样本。

- 为了不改变原数据的分布，GOSS在保留下来的小梯度样本上乘一个放大系数，以弥补被丢弃的小梯度样本。

### Exclusive Feature Bundling(EFB)： 
基于互斥特征的特征捆绑。该方法用于减少样本的特征。

- 传统特征选取方法基于PCA的原理，认为许多特征包含重复信息，并以此选择重要特征或使用新特征，但实际场景往往难以使用。

- EFB根据特征间的互斥性来将互斥的特征打包成一个新的，信息密度更大的特征。具体的如果对于所有样本，两个特征都不会同时为非零值，即认为两个特征互斥，实际中如果只有少量样本不满足也可以认为互斥。

## 优化
### 直方图
#### 优点：
- 节省空间：需要存储的信息更少
- 节省时间：分割点减少，也不用预排序
- 可能还自带正则化效果

#### 缺点
- 分割点不是很精确

### leaf-wise 生长策略
- 相对level-wise可以避免很多不必要的分裂；
- 容易过拟合一些，需要限制最大深度。

### 直方图做差加速
通常构造直方图，需要遍历该叶子上的所有数据。但是事实上一个叶子的直方图可以由它的父亲结点的直方图与它兄弟的直方图做差得到。

LightGBM在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。

### 直接支持 categorical 特征


## 参考
- http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/7_GBT.html



